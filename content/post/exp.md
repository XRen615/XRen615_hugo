+++
draft = false
tags = ["experiments","A/B Testing"]
date = "2018-04-13T01:30:00+08:00"
author = "X.Ren"
comments = true
share = true
slug = "experiment"
title = "Random thoughts on A/B testing at LinkedIn"

+++

<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

**写在前面**

Random thoughts about A/B testing came into my mind while working at LinkedIn.  

A/B测试正被越来越多的公司和科研机构使用，它能让决策告别『拍脑袋』，以实际的试验结果对比展示各个方案间的取舍，从而实现数据驱动的决策。LinkedIn作为业界最早以及最广泛使用A/B测试的科技公司之一，在方法论和工程实现上都有丰富的积累。本篇文章并非对其完整地论述，而是在日常工作中产生的随机总结和思考。  

<div  align="center">    
<img src="http://7xro3y.com1.z0.glb.clouddn.com/strata.jpeg" width = "500" height = "350"/>  
</div>  

***  

### Statistical ABC  

本章节分级总结A/B测试的统计学基础。简单明了为先，严谨其次。

**LEVEL1: 告别『拍脑袋』**

这是树立数据驱动思想的第一步：开发者和产品经理都开始意识到，小至用绿色图标还是红色图标，大到内容流的构成和排序，都不能以『拍脑袋』的方式来决定：这种方法并不能帮助他们找到最优解，且倘若彼此意见不一致，彼此都很难说服对方。

于是他们产生了朴素的想法：给一半人展示绿色图标，同时给另一半人展示红色图标，然后比较两组试验的表现 —— 这就是A/B测试的基本思想。这样一来，A/B测试不仅可以回答『哪种方案更好？』而且可以通过比较定量地分析『好多少？』  

![rog](http://7xro3y.com1.z0.glb.clouddn.com/redOrGreen.png "red or green?")

**LEVEL2: 我的差异显著吗**  

现在通过试验，产品经理喜爱的绿色和工程师喜爱的红色页面拥有了不同的表现数据。譬如点击率（CTP）对比是10% vs. 11% —— 那么，这足以说明后者更受用户欢迎吗？

除了关心试验数据本身，我们还需要考虑A/B测试结果中产生的组间差异有多大可能来自于随机误差。试想，同样是10% vs. 11%，10/100 vs. 11/100的说服力与1000/10000 vs. 1100/10000的说服力一样吗？直觉告诉我们，样本高达10000时显示出的1%差异似乎更有说服力，而100样本量实验中的仅仅1个转化的差异似乎有更大的可能来自于随机误差。

换而言之，在产品决策者做出决定之前，不仅需要知道实验组与对照组对比表现有多大提升，同时也需要知道观察到的提升有多大可能是来源于试验中的随机误差。

统计学中的假设检验理论为我们提供了量化这种可能性的方法。暂时略去其背后的理论体系，从实用的角度看，现在任何成熟的A/B测试系统都会随着试验结果一并给出一个叫做**p-value**的统计量，它将告诉我们两组实验数据的差异来自于随机误差的概率。一般地，行业标准可以接受的p-value范围是 <=0.05，换而言之，我们至少有95%的信心推断实验数据的差异确实来源于产品改动而非随机误差 —— 这时我们称组间差异是**统计显著**的。   

**LEVEL3 计算关键指标**  

对于不专门从事数据工作的职位，理解LEVEL2 已经达到了了解A/B测试的基础目标。然而数据分析师仍需要知其所以然，掌握各个关键指标背后的统计计算逻辑，才能有理有据地对实验数据进行详细分析。

仍然以点击率（CTP）为例，假设我们已知实验组的样本总量和点击量分别为（Nt, Xt），对照组的样本总量和点击量分别为（Nc, Xc），如何评价本次试验的显著性？

首先，可以很直观地计算出两个组别在实验中的点击率   

$$Pt = \frac {Nt}{Xt} \quad and \quad Pc = \frac {Nc}{Xc}$$  

我们进行原假设为『实验组和对照组的点击率没有明显差别』的假设检验，即
$$H_0: Pt - Pc = 0$$  

因为用户点击与否是一个二元选择的事件，我们可以认为用户点击率P符合二项分布B(n,p)，在原假设下实验组和对照组来自同一总体，拥有相同的方差sigma。那么，根据中心极限定理，Pt - Pc 将近似符合均值为0，方差为
$$\frac {\sigma^2}{Nt} + \frac {\sigma^2}{Nc}$$  
的正态分布。即  
$$Pt - Pc \Rightarrow N (0, \frac {\sigma^2}{Nt} + \frac {\sigma^2}{Nc})$$  

其中, sigma将使用实验组与对照组的[*合并方差 (pooled variance)*](https://en.wikipedia.org/wiki/Pooled_variance) 来进行估计，它在数值上等于将实验组与对照组所有*留数 (residuals)*的平方和平均分到每个自由度中，即

$$\sigma^2 = S_{pool}^2 = \frac {\sum({Xc - \overline{Xc}})^2 +  \sum({Xt - \overline{Xt}})^2} {Nc + Nt -2}$$

式中自由度Nt+Nc-2来源于[*Bessel's Correction*](https://en.wikipedia.org/wiki/Bessel%27s_correction)
那么，当  

$$abs (Pt - Pc )> 1.96*\sqrt{S_{pool}(\frac {1}{Nt} + \frac {1}{Nc})}$$  

时，我们即可有95%的置信度推断实验组与对照组的差距不是来源于随机误差。  

![95%](http://7xro3y.com1.z0.glb.clouddn.com/normal_distribution.png "95% confidence interval")  

*在实际的计算中，也常用经验公式   

$$S_{pool}^2 = P(1-P)(\frac {1}{Nt} + \frac {1}{Nc}) \quad where$$

$$P = P_{pool} = \frac {(Xt + Sc)} {(Nt + Nc)}$$

来计算Spool  

**LEVEL4 正确解读实验结果**  

了解A/B测试的关键指标p-value的计算逻辑使得数据分析人员可以对实验结果进行基本的判断。但是要科学合理地综合解读A/B测试的各项结果并避开一些常见误区，仍需了解许多其他关键的要素。LEVEL4将介绍A/B测试的一些进阶概念，包括检验效力（Power），最大效力实验（Max Power Ramp, MPR），多重检验偏差（Multi-testing problem）的统计原理及它们对实验可能造成的影响。

*检验效力（Power）*

在LEVEL 3中我们使用了假设检验来判断实验组和对照组的差距以统计学的观点是否可信。在实质上，这种检验是在衡量两组数据的实际差异与其对应方差的相对大小。直观地讲，在方差一定的情况下，实际差异越大越可信；在实际差异一定的情况下，方差越小越可信。数学上，该假设检验建立在*t统计量*上：

$$t = \frac {\Delta}{std(\Delta)} = \frac {\Delta}{\sqrt{(S_{pool}^2)(\frac {1}{Nt} + \frac {1}{Nc})}} $$  

随着t增大，实验组和对照组之间的差异来自于随机误差的可能性就越小。当t大于某一阈值以后，我们认为已经达到了足够的置信度，即所谓的“统计显著”。那么，假设我的两个组别间确实存在差异，在对应的实验条件下计算出来的t统计量是否一定会大于这个阈值呢？答案是否定的，即在实际实验中存在“确实有差异但检测不出”的情况。因此，我们把”在实验组与对照组确实存在差异的情形下，假设检验返回统计显著”的概率称之为“检验效力（power）”。数学上，我们定义  

$$power = P (|t| > Z_{\alpha} | E(\Delta) = \delta)  $$  

可以直观的看出，对于一定量的差异，提高检验效力最好的方法是降低取样方差std(),这可以通过收集更大的样本量N来实现。对于检验效力，业界普遍接受的标准是0.8，即在有实际差异的前提下，检出率在80%以上。  

*最大效力实验（Maximum Power Ramp, MPR)*

了解了检验效力以后，我们在实际的产品测试中经常面临的问题是：在样本总量有限的情况下，如何分配实验组与对照组能使检出统计显著的几率最大？结论是50% vs. 50%，即实验组与对照组对半分配的实验设计拥有最大的检验效力，因此这样的实验配置也被也被称作最大效力实验（Maximum Power Ramp, MPR)。

略去严密的证明，可以用以下事实在数学上简单理解为何50% vs. 50%的实验检出效力最大。在实验总样本量一定的情况下，即  

$$Nt+Nc= constant$$  

容易证明当且仅当

$$Nt=Nc = \frac{1}{2}constant$$  

时，式  
$$\frac {1}{Nt} + \frac {1}{Nc}$$  

取得极小值，这意味着  

$$t = \frac {\Delta}{std(\Delta)} = \frac {\Delta}{\sqrt{(S_{pool}^2)(\frac {1}{Nt} + \frac {1}{Nc})}} $$  

在给定的delta下取得极大值，也就意味着检出效力最大。

在实际的实验中，这意味着50%Ramp对于效果的评价拥有最大的发言权，即不论改变是好是坏，它在50%的Ramp中被发现的几率都会达到最高。因此，如果50%的实验仍然没有达到满意的检出效力，我们能做的只能是等待更长的实验周期以进一步扩大样本量N，类似『继续向上Ramp e.g.到80%，期待收到更明显效果』的想法都是错误的。

*多重检验偏差（Multi-testing problem）*

Power的存在导致我们需要考虑『看上去不显著的实验结果是否真的代表没有差异』，是实际的实验中，我们在分析实验结果，尤其是对于大型实验，常常还需要考虑『看上去显著的实验结果是否真的代表有差异』。

如前所述，我们进行的假设检验做出显著结论的标准是『有95%的置信度推断实验组与对照组的差距不是来源于随机误差』，从反面看，这仍然代表着『实验组与对照组的差距来源于随机误差的可能性为5%』。如果我们仅关注单一指标，这样的假阳性检出率不会带来大的问题。但对于一些大型的实验平台，我们可能会同时看到几十个乃至数百个指标的对照结果。那么，即使在假设检验被正确执行的情况下，5%的假阳性（false-positive）率仍然可能直接影响我们的判断。

例如在LinkedIn，大型的实验平台XLNT为我们同时提供一百个指标的实验结果，作为数据分析人员仍要时刻警惕在这一百个指标中藏着五个假阳性（即因随机误差而产生统计显著结论）是完全可能且合理的。  

![metrics](http://7xro3y.com1.z0.glb.clouddn.com/metrics.png "When you look at 100 metrics at one time...")  
http://7xro3y.com1.z0.glb.clouddn.com/metrics.png
那么，如何避免在做出决策时受到多重检验偏差的误导？有两个有效的解决方案，

- 由于5%的误差是随机产生的，重复一次实验，大概率不会出现在同样的结果
- 在实验前事先选定本次试验最有可能影响的指标作为成功指标，并在实验后重点关注该指标的表现来判断改动的效果-而不是先做实验，然后再根据数百个指标的升降来做出决定。这样做的好处是显而易见的，多重检验偏差造成的假阳性检出刚好落在事先选定的那（几）个指标上的概率是很小的。

从检验效力和多重检验偏差这两个概念，可以深刻的体会到A/B测试是一套建立在统计和概率基础上的科学体系。在这个体系中，我们做的一切是为了缩小随机性以帮助决策，但绝非消灭随机性。所以，如何如何在假阴性和假阳性之间做出取舍，正确地分析和解读实验结果，对实验的有效性至关重要。  

**LEVEL5 实验debug (WIP)**

- hashid的工作原理
- Lix kafka tracking frame
- 样本比失衡（Sample Size Ratio Mismatch, SSRM）
